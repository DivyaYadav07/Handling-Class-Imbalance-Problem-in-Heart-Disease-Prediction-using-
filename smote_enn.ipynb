{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJrAeUQ8CbBG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying SMOTE-ENN on combined Heart Disease Dataset and performance comparison before and after applying SMOTE_ENN\n",
        "\n",
        "\n",
        "1.   List item please download the Heart disease combined dataset from this https://www.kaggle.com/datasets/mfarhaannazirkhan/heart-dataset/data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kCsL6wNyRyh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, jaccard_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.combine import SMOTEENN\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import shap\n",
        "from itertools import combinations\n",
        "\n",
        "# Load the cleaned dataset from your drive\n",
        "cleaned_df = pd.read_csv('/content/drive/MyDrive/Datasets/heart_disease/cleaned_merged_heart_dataset.csv')\n",
        "\n",
        "# Separate the 'target' column (target variable) from the features\n",
        "target = cleaned_df['target']  # Extract the target column\n",
        "features = cleaned_df.drop(['target'], axis=1)  # Remove 'target' from features\n",
        "\n",
        "# Normalize the feature columns\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(features)  # Normalize only the feature columns\n",
        "\n",
        "# Convert the normalized features back to a DataFrame and restore column names\n",
        "normalized_features = pd.DataFrame(normalized_features, columns=features.columns)\n",
        "\n",
        "# Recombine the target column with the normalized features\n",
        "cleaned_df = pd.concat([normalized_features, target.reset_index(drop=True)], axis=1)\n",
        "\n",
        "total_rows = cleaned_df.shape[0]\n",
        "print(\"Total number of rows:\", total_rows)\n",
        "\n",
        "# Select relevant features\n",
        "temp_df = cleaned_df.drop(['sex', 'trestbps', 'chol', 'fbs', 'exang', 'slope', 'ca', 'restecg', 'age'], axis=1).copy()\n",
        "\n",
        "# Create a copy for testing\n",
        "cleaned_df_copy = temp_df.copy()\n",
        "\n",
        "# Function to calculate derived features\n",
        "def calculate_derived_features(df):\n",
        "    feature_cols = ['cp', 'thalachh', 'oldpeak', 'thal']\n",
        "    combined_features = pd.DataFrame()\n",
        "    pairs = list(combinations(feature_cols, 2))\n",
        "    for idx, (n1, n2) in enumerate(pairs, 1):\n",
        "        max_val = df[[n1, n2]].max(axis=1)\n",
        "        min_val = df[[n1, n2]].min(axis=1)\n",
        "        combined_features[f'S1_P{idx}'] = np.log10(np.where(max_val > 0, max_val, 1e-6))\n",
        "        combined_features[f'S2_P{idx}'] = np.log10(np.where(min_val > 0, min_val, 1e-6))\n",
        "        combined_features[f'S3_P{idx}'] = max_val - min_val\n",
        "        combined_features[f'S4_P{idx}'] = max_val * min_val\n",
        "        combined_features[f'S5_P{idx}'] = max_val + min_val\n",
        "        combined_features[f'S6_P{idx}'] = max_val / (min_val + 1e-6)\n",
        "    return combined_features\n",
        "\n",
        "# Apply the function to the data\n",
        "combined_features = calculate_derived_features(cleaned_df_copy)\n",
        "cleaned_df_copy = pd.concat([cleaned_df_copy, combined_features], axis=1)\n",
        "\n",
        "# Drop original features to avoid redundancy\n",
        "#cleaned_df_copy = cleaned_df_copy.drop(['cp','thal','thalachh','oldpeak','S1_C1','S2_C1','S3_C1','S4_C1','S5_C1','S6_C1','S1_C2','S2_C2','S3_C2','S4_C2','S5_C2','S6_C2', 'S1_C3','S2_C3','S3_C3','S4_C3','S5_C3','S6_C3','S1_C4','S2_C4','S3_C4','S4_C4','S5_C4','S6_C4','S1_C5','S2_C5','S3_C5','S4_C5','S5_C5','S6_C5'],axis = 1).copy()\n",
        "#cleaned_df_copy = cleaned_df_copy.drop(['cp', 'thal', 'thalachh', 'oldpeak'], axis=1).copy()\n",
        "\n",
        "# Prepare features and target\n",
        "X = cleaned_df_copy.drop(['target'], axis=1).values\n",
        "y = cleaned_df_copy['target'].values\n",
        "\n",
        "# Split the data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the dataset(Remark: No need to standardize data here if earlier we did)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print class distribution before SMOTE-ENN\n",
        "print(\"\\nClass Distribution Before SMOTE-ENN (Training Set):\")\n",
        "class_counts_before = pd.Series(y_train).value_counts()\n",
        "class_proportions_before = pd.Series(y_train).value_counts(normalize=True)\n",
        "print(\"Counts:\\n\", class_counts_before)\n",
        "print(\"Proportions:\\n\", class_proportions_before)\n",
        "\n",
        "# Define classifiers\n",
        "models = [\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
        "    ('DT', DecisionTreeClassifier()),\n",
        "    ('NB', GaussianNB()),\n",
        "    ('RF', RandomForestClassifier(n_estimators=100)),\n",
        "    ('ABC', AdaBoostClassifier(n_estimators=100)),\n",
        "    ('GB', GradientBoostingClassifier()),\n",
        "    ('LR', LogisticRegression()),\n",
        "    ('SVM', SVC(probability=True))\n",
        "]\n",
        "\n",
        "# Function to evaluate models and return metrics\n",
        "def evaluate_models(models, X_train, X_test, y_train, y_test, title_prefix=\"\"):\n",
        "    fitted_models = {}\n",
        "    metrics = {\n",
        "        \"Model\": [],\n",
        "        \"Accuracy\": [],\n",
        "        \"Precision\": [],\n",
        "        \"Recall\": [],\n",
        "        \"F1 Score\": [],\n",
        "        \"AUC-ROC\": [],\n",
        "        \"Jaccard Score\": []\n",
        "    }\n",
        "\n",
        "    # Train and evaluate individual models\n",
        "    for name, model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "        fitted_models[name] = model\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "        metrics[\"Model\"].append(name)\n",
        "        metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Precision\"].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"Recall\"].append(recall_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"F1 Score\"].append(f1_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"AUC-ROC\"].append(roc_auc_score(y_test, y_pred_prob))\n",
        "        metrics[\"Jaccard Score\"].append(jaccard_score(y_test, y_pred))\n",
        "\n",
        "    # Voting Classifier\n",
        "    voting_clf = VotingClassifier(estimators=models, voting='soft')\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    y_pred_voting = voting_clf.predict(X_test)\n",
        "    y_pred_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    metrics[\"Model\"].append(\"Voting Classifier (Soft Voting)\")\n",
        "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred_voting))\n",
        "    metrics[\"Precision\"].append(precision_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"Recall\"].append(recall_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"AUC-ROC\"].append(roc_auc_score(y_test, y_pred_prob_voting))\n",
        "    metrics[\"Jaccard Score\"].append(jaccard_score(y_test, y_pred_voting))\n",
        "\n",
        "    # Convert metrics to DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "    print(f\"\\n{title_prefix} Metrics:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Plot ROC Curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, model in fitted_models.items():\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "    y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    fpr_voting, tpr_voting, _ = roc_curve(y_test, y_pred_prob_voting)\n",
        "    roc_auc_voting = auc(fpr_voting, tpr_voting)\n",
        "    plt.plot(fpr_voting, tpr_voting, label=f'Voting Classifier (AUC = {roc_auc_voting:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.title(f'{title_prefix} ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{title_prefix.lower().replace(\" \", \"_\")}_roc_curve.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Precision-Recall Curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for name, model in fitted_models.items():\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "        ap_score = average_precision_score(y_test, y_prob)\n",
        "        plt.plot(recall, precision, label=f'{name} (AP = {ap_score:.2f})')\n",
        "    y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    precision_voting, recall_voting, _ = precision_recall_curve(y_test, y_prob_voting)\n",
        "    ap_score_voting = average_precision_score(y_test, y_prob_voting)\n",
        "    plt.plot(recall_voting, precision_voting, label=f'Voting Classifier (AP = {ap_score_voting:.2f})')\n",
        "    plt.title(f'{title_prefix} Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{title_prefix.lower().replace(\" \", \"_\")}_pr_curve.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return fitted_models, metrics_df\n",
        "\n",
        "# Evaluate models on original data\n",
        "print(\"Evaluating models on original data...\")\n",
        "original_models, original_metrics = evaluate_models(models, X_train_scaled, X_test_scaled, y_train, y_test, \"Original Data\")\n",
        "\n",
        "# Apply SMOTE-ENN\n",
        "smote_enn = SMOTEENN(random_state=100)\n",
        "X_train_smote_enn, y_train_smote_enn = smote_enn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Print class distribution after SMOTE-ENN\n",
        "print(\"\\nClass Distribution After SMOTE-ENN (Training Set):\")\n",
        "class_counts_after = pd.Series(y_train_smote_enn).value_counts()\n",
        "class_proportions_after = pd.Series(y_train_smote_enn).value_counts(normalize=True)\n",
        "print(\"Counts:\\n\", class_counts_after)\n",
        "print(\"Proportions:\\n\", class_proportions_after)\n",
        "\n",
        "# Evaluate models on SMOTE-ENN balanced data\n",
        "print(\"\\nEvaluating models on SMOTE-ENN balanced data...\")\n",
        "smote_enn_models, smote_enn_metrics = evaluate_models(models, X_train_smote_enn, X_test_scaled, y_train_smote_enn, y_test, \"SMOTE-ENN Balanced Data\")\n",
        "\n",
        "# Feature Importance and SHAP Analysis (for Random Forest on SMOTE-ENN data)\n",
        "rf_model = smote_enn_models['RF']\n",
        "feature_names = cleaned_df_copy.drop(['target'], axis=1).columns\n",
        "importances = rf_model.feature_importances_\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.barh(feature_names, importances, color='green')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance (Random Forest - SMOTE-ENN)\")\n",
        "plt.savefig('feature_importance_smote_enn.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# SHAP Analysis\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = explainer.shap_values(X_test_scaled)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    print(f\"SHAP summary for Class {class_idx} (SMOTE-ENN)\")\n",
        "    custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"blue\"])\n",
        "    shap.summary_plot(shap_values[:, :, class_idx], X_test_scaled, feature_names=feature_names, plot_type=\"bar\", cmap=custom_cmap(0.5))\n"
      ],
      "metadata": {
        "id": "pk2A5HlZCmGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying SMOTE on combined heart disease dataset and performance comparison before and after applying SMOTE for data balancing."
      ],
      "metadata": {
        "id": "i6BX6Eg_Tj7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, jaccard_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import shap\n",
        "from itertools import combinations\n",
        "\n",
        "# Load the cleaned dataset\n",
        "cleaned_df = pd.read_csv('/content/drive/MyDrive/Datasets/heart_disease/cleaned_merged_heart_dataset.csv')\n",
        "\n",
        "# Separate the 'target' column (target variable) from the features\n",
        "target = cleaned_df['target']  # Extract the target column\n",
        "features = cleaned_df.drop(['target'], axis=1)  # Remove 'target' from features\n",
        "\n",
        "# Normalize the feature columns\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(features)  # Normalize only the feature columns\n",
        "\n",
        "# Convert the normalized features back to a DataFrame and restore column names\n",
        "normalized_features = pd.DataFrame(normalized_features, columns=features.columns)\n",
        "\n",
        "# Recombine the target column with the normalized features\n",
        "cleaned_df = pd.concat([normalized_features, target.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Select relevant features\n",
        "temp_df = cleaned_df.drop(['sex', 'trestbps', 'chol', 'fbs', 'exang', 'slope', 'ca', 'restecg', 'age'], axis=1).copy()\n",
        "\n",
        "# Create a copy for testing\n",
        "cleaned_df_copy = temp_df.copy()\n",
        "\n",
        "# Function to calculate derived features\n",
        "def calculate_derived_features(df):\n",
        "    feature_cols = ['cp', 'thalachh', 'oldpeak', 'thal']\n",
        "    combined_features = pd.DataFrame()\n",
        "    pairs = list(combinations(feature_cols, 2))\n",
        "    for idx, (n1, n2) in enumerate(pairs, 1):\n",
        "        max_val = df[[n1, n2]].max(axis=1)\n",
        "        min_val = df[[n1, n2]].min(axis=1)\n",
        "        combined_features[f'S1_P{idx}'] = np.log10(np.where(max_val > 0, max_val, 1e-6))\n",
        "        combined_features[f'S2_P{idx}'] = np.log10(np.where(min_val > 0, min_val, 1e-6))\n",
        "        combined_features[f'S3_P{idx}'] = max_val - min_val\n",
        "        combined_features[f'S4_P{idx}'] = max_val * min_val\n",
        "        combined_features[f'S5_P{idx}'] = max_val + min_val\n",
        "        combined_features[f'S6_P{idx}'] = max_val / (min_val + 1e-6)\n",
        "    return combined_features\n",
        "\n",
        "# Apply the function to the data\n",
        "combined_features = calculate_derived_features(cleaned_df_copy)\n",
        "cleaned_df_copy = pd.concat([cleaned_df_copy, combined_features], axis=1)\n",
        "\n",
        "# Drop original features to avoid redundancy\n",
        "#cleaned_df_copy = cleaned_df_copy.drop(['cp','thal','thalachh','oldpeak','S1_C1','S2_C1','S3_C1','S4_C1','S5_C1','S6_C1','S1_C2','S2_C2','S3_C2','S4_C2','S5_C2','S6_C2', 'S1_C3','S2_C3','S3_C3','S4_C3','S5_C3','S6_C3','S1_C4','S2_C4','S3_C4','S4_C4','S5_C4','S6_C4','S1_C5','S2_C5','S3_C5','S4_C5','S5_C5','S6_C5'],axis = 1).copy()\n",
        "cleaned_df_copy = cleaned_df_copy.drop(['cp', 'thal', 'thalachh', 'oldpeak'], axis=1).copy()\n",
        "\n",
        "# Prepare features and target\n",
        "X = cleaned_df_copy.drop(['target'], axis=1).values\n",
        "y = cleaned_df_copy['target'].values\n",
        "\n",
        "# Split the data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print class distribution before SMOTE\n",
        "print(\"\\nClass Distribution Before SMOTE (Training Set):\")\n",
        "class_counts_before = pd.Series(y_train).value_counts()\n",
        "class_proportions_before = pd.Series(y_train).value_counts(normalize=True)\n",
        "print(\"Counts:\\n\", class_counts_before)\n",
        "print(\"Proportions:\\n\", class_proportions_before)\n",
        "\n",
        "# Define classifiers\n",
        "models = [\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
        "    ('DT', DecisionTreeClassifier()),\n",
        "    ('NB', GaussianNB()),\n",
        "    ('RF', RandomForestClassifier(n_estimators=100)),\n",
        "    ('ABC', AdaBoostClassifier(n_estimators=100)),\n",
        "    ('GB', GradientBoostingClassifier()),\n",
        "    ('LR', LogisticRegression()),\n",
        "    ('SVM', SVC(probability=True))\n",
        "]\n",
        "\n",
        "# Function to evaluate models and return metrics\n",
        "def evaluate_models(models, X_train, X_test, y_train, y_test, title_prefix=\"\"):\n",
        "    fitted_models = {}\n",
        "    metrics = {\n",
        "        \"Model\": [],\n",
        "        \"Accuracy\": [],\n",
        "        \"Precision\": [],\n",
        "        \"Recall\": [],\n",
        "        \"F1 Score\": [],\n",
        "        \"AUC-ROC\": [],\n",
        "        \"Jaccard Score\": []\n",
        "    }\n",
        "\n",
        "    # Train and evaluate individual models\n",
        "    for name, model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "        fitted_models[name] = model\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "        metrics[\"Model\"].append(name)\n",
        "        metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Precision\"].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"Recall\"].append(recall_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"F1 Score\"].append(f1_score(y_test, y_pred, zero_division=0))\n",
        "        metrics[\"AUC-ROC\"].append(roc_auc_score(y_test, y_pred_prob))\n",
        "        metrics[\"Jaccard Score\"].append(jaccard_score(y_test, y_pred))\n",
        "\n",
        "    # Voting Classifier\n",
        "    voting_clf = VotingClassifier(estimators=models, voting='soft')\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    y_pred_voting = voting_clf.predict(X_test)\n",
        "    y_pred_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    metrics[\"Model\"].append(\"Voting Classifier (Soft Voting)\")\n",
        "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred_voting))\n",
        "    metrics[\"Precision\"].append(precision_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"Recall\"].append(recall_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred_voting, zero_division=0))\n",
        "    metrics[\"AUC-ROC\"].append(roc_auc_score(y_test, y_pred_prob_voting))\n",
        "    metrics[\"Jaccard Score\"].append(jaccard_score(y_test, y_pred_voting))\n",
        "\n",
        "    # Convert metrics to DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "    print(f\"\\n{title_prefix} Metrics:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Plot ROC Curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, model in fitted_models.items():\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "    y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    fpr_voting, tpr_voting, _ = roc_curve(y_test, y_pred_prob_voting)\n",
        "    roc_auc_voting = auc(fpr_voting, tpr_voting)\n",
        "    plt.plot(fpr_voting, tpr_voting, label=f'Voting Classifier (AUC = {roc_auc_voting:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.title(f'{title_prefix} ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{title_prefix.lower().replace(\" \", \"_\")}_roc_curve.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Precision-Recall Curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for name, model in fitted_models.items():\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "        ap_score = average_precision_score(y_test, y_prob)\n",
        "        plt.plot(recall, precision, label=f'{name} (AP = {ap_score:.2f})')\n",
        "    y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "    precision_voting, recall_voting, _ = precision_recall_curve(y_test, y_prob_voting)\n",
        "    ap_score_voting = average_precision_score(y_test, y_prob_voting)\n",
        "    plt.plot(recall_voting, precision_voting, label=f'Voting Classifier (AP = {ap_score_voting:.2f})')\n",
        "    plt.title(f'{title_prefix} Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{title_prefix.lower().replace(\" \", \"_\")}_pr_curve.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return fitted_models, metrics_df\n",
        "\n",
        "# Evaluate models on original data\n",
        "print(\"Evaluating models on original data...\")\n",
        "original_models, original_metrics = evaluate_models(models, X_train_scaled, X_test_scaled, y_train, y_test, \"Original Data\")\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Print class distribution after SMOTE\n",
        "print(\"\\nClass Distribution After SMOTE (Training Set):\")\n",
        "class_counts_after = pd.Series(y_train_smote).value_counts()\n",
        "class_proportions_after = pd.Series(y_train_smote).value_counts(normalize=True)\n",
        "print(\"Counts:\\n\", class_counts_after)\n",
        "print(\"Proportions:\\n\", class_proportions_after)\n",
        "\n",
        "# Evaluate models on SMOTE balanced data\n",
        "print(\"\\nEvaluating models on SMOTE balanced data...\")\n",
        "smote_models, smote_metrics = evaluate_models(models, X_train_smote, X_test_scaled, y_train_smote, y_test, \"SMOTE Balanced Data\")\n",
        "\n",
        "# Feature Importance and SHAP Analysis (for Random Forest on SMOTE data)\n",
        "rf_model = smote_models['RF']\n",
        "feature_names = cleaned_df_copy.drop(['target'], axis=1).columns\n",
        "importances = rf_model.feature_importances_\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.barh(feature_names, importances, color='green')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance (Random Forest - SMOTE)\")\n",
        "plt.savefig('feature_importance_smote.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# SHAP Analysis\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = explainer.shap_values(X_test_scaled)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    print(f\"SHAP summary for Class {class_idx} (SMOTE)\")\n",
        "    custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"blue\"])\n",
        "    shap.summary_plot(shap_values[:, :, class_idx], X_test_scaled, feature_names=feature_names, plot_type=\"bar\", cmap=custom_cmap(0.5))\n"
      ],
      "metadata": {
        "id": "xVGpvh52CmAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Mr5viCnCl9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdYzUot0Cl6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXy-ogX4Cl2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}